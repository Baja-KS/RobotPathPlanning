{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25157db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, map_height, map_width, max_num_obstacles):\n",
    "        self.map_height = map_height\n",
    "        self.map_width = map_width\n",
    "        self.max_num_obstacles = max_num_obstacles\n",
    "        self.map = np.ones((map_height, map_width), dtype='int')\n",
    "        self.map[0:2, :] = 0\n",
    "        self.map[:, 0:2] = 0\n",
    "        self.map[map_height-2:, :] = 0\n",
    "        self.map[:, map_width - 2:] = 0\n",
    "        \n",
    "        self.map_copy = np.array(self.map)\n",
    "        \n",
    "        print(self.map)\n",
    "        \n",
    "        self.init_env(self.max_num_obstacles)\n",
    "        \n",
    "        self.agent_pos = [0, 0]   \n",
    "        self.target_pos = [map_height - 2, map_width - 2]\n",
    "    \n",
    "    def reset_env(self):\n",
    "        self.map = np.array(self.map_copy)\n",
    "        self.init_env(np.random.randint(0, self.max_num_obstacles + 1))\n",
    "        self.init_agent_pos()\n",
    "        self.init_target_pos()\n",
    "        \n",
    "    def init_env(self, num_obstacles):\n",
    "        for _ in range(num_obstacles):\n",
    "            obstacle_pos = [0, 0]\n",
    "            while self.map[obstacle_pos[0], obstacle_pos[1]] == 0:\n",
    "                obstacle_pos[0] = np.random.randint(2, self.map_height - 2)\n",
    "                obstacle_pos[1] = np.random.randint(2, self.map_width - 2)\n",
    "            self.map[obstacle_pos[0], obstacle_pos[1]] = 0\n",
    "    \n",
    "    def init_agent_pos(self):\n",
    "        self.agent_pos[0] = np.random.randint(2, self.map_height - 2)\n",
    "        self.agent_pos[1] = np.random.randint(2, self.map_width - 2)\n",
    "        \n",
    "        while(self.map[self.agent_pos[0], self.agent_pos[1]] == 0):\n",
    "            self.agent_pos[0] = np.random.randint(2, self.map_height - 2)\n",
    "            self.agent_pos[1] = np.random.randint(2, self.map_width - 2)\n",
    "    \n",
    "    def init_target_pos(self):\n",
    "        self.target_pos[0] = np.random.randint(2, self.map_height - 2)\n",
    "        self.target_pos[1] = np.random.randint(2, self.map_width - 2)\n",
    "        \n",
    "        while(self.map[self.target_pos[0], self.target_pos[1]] == 0 or \\\n",
    "              self.target_pos[0] == self.agent_pos[0] and self.target_pos[1] == self.agent_pos[1]):\n",
    "            self.target_pos[0] = np.random.randint(2, self.map_height - 2)\n",
    "            self.target_pos[1] = np.random.randint(2, self.map_width - 2)\n",
    "            \n",
    "    def get_view(self):\n",
    "        view = np.zeros(8, dtype='int').tolist()\n",
    "\n",
    "        view[0:3] = self.map[self.agent_pos[0] - 1][self.agent_pos[1] - 1:self.agent_pos[1] + 2]\n",
    "\n",
    "        view[3] = self.map[self.agent_pos[0]][self.agent_pos[1] - 1]\n",
    "        view[4] = self.map[self.agent_pos[0]][self.agent_pos[1] + 1]\n",
    "\n",
    "        view[5:8] = self.map[self.agent_pos[0] + 1][self.agent_pos[1] - 1:self.agent_pos[1] + 2]\n",
    "\n",
    "        return view\n",
    "    \n",
    "    def is_terminal_state(self):\n",
    "        if self.map[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "            return True\n",
    "        if self.target_pos[0] == self.agent_pos[0] and self.target_pos[1] == self.agent_pos[1]:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def step(self, action, prev_action=4):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        looping = False\n",
    "        \n",
    "        if action == 0:\n",
    "            self.agent_pos[0] -= 1\n",
    "            if prev_action == 4:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 1:\n",
    "            self.agent_pos[0] -= 1\n",
    "            self.agent_pos[1] += 1\n",
    "            if prev_action == 5:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 2:\n",
    "            self.agent_pos[0] += 1\n",
    "            if prev_action == 6:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 3:\n",
    "            self.agent_pos[0] += 1\n",
    "            self.agent_pos[1] += 1\n",
    "            if prev_action == 7:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 4:\n",
    "            self.agent_pos[0] += 1\n",
    "            if prev_action == 0:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 5:\n",
    "            self.agent_pos[0] += 1\n",
    "            self.agent_pos[1] -= 1\n",
    "            if prev_action == 1:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 6:\n",
    "            self.agent_pos[1] -= 1\n",
    "            if prev_action == 2:\n",
    "                looping = True\n",
    "                \n",
    "        elif action == 7:\n",
    "            self.agent_pos[0] -= 1\n",
    "            self.agent_pos[1] -= 1\n",
    "            if prev_action == 3:\n",
    "                looping = True\n",
    "        \n",
    "        reward -= 1\n",
    "        \n",
    "        if looping:\n",
    "            reward -= 2\n",
    "        \n",
    "        new_view = env.get_view()\n",
    "        \n",
    "        if self.map[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "            reward -= 1000\n",
    "            done = True\n",
    "        \n",
    "        if self.target_pos[0] == self.agent_pos[0] and self.target_pos[1] == self.agent_pos[1]:\n",
    "            reward += 100\n",
    "            done = True\n",
    "        \n",
    "        return new_view, self.agent_pos, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, map_height, map_width, eps, eps_decay, min_eps, lr, lr_decay, gamma):\n",
    "        self.map_height = map_height\n",
    "        self.map_width = map_width\n",
    "        self.eps = eps\n",
    "        self.min_eps = min_eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.lr = lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.actions = ['u', 'ur', 'r', 'dr', 'd', 'dl', 'l', 'ul']\n",
    "        \n",
    "        dims = []\n",
    "        for i in range(8):\n",
    "            dims.append(2)\n",
    "        dims.append(self.map_height)\n",
    "        dims.append(self.map_width)\n",
    "        dims.append(self.map_height)\n",
    "        dims.append(self.map_width)\n",
    "        dims.append(len(self.actions) + 1)\n",
    "        dims.append(len(self.actions))\n",
    "        \n",
    "        self.q_values = np.zeros(dims)\n",
    "        \n",
    "        self.q_values[:, 0, :, :, :, :, :, :, :, :, :, :, :, 0] = -100000\n",
    "        self.q_values[:, :, 0, :, :, :, :, :, :, :, :, :, :, 1] = -100000\n",
    "        self.q_values[:, :, :, :, 0, :, :, :, :, :, :, :, :, 2] = -100000\n",
    "        self.q_values[:, :, :, :, :, :, :, 0, :, :, :, :, :, 3] = -100000\n",
    "        self.q_values[:, :, :, :, :, :, 0, :, :, :, :, :, :, 4] = -100000\n",
    "        self.q_values[:, :, :, :, :, 0, :, :, :, :, :, :, :, 5] = -100000\n",
    "        self.q_values[:, :, :, 0, :, :, :, :, :, :, :, :, :, 6] = -100000\n",
    "        self.q_values[0, :, :, :, :, :, :, :, :, :, :, :, :, 7] = -100000\n",
    "        \n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 0, 4] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 1, 5] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 2, 6] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 3, 7] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 4, 0] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 5, 1] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 6, 2] = -1000\n",
    "        self.q_values[:, :, :, :, :, :, :, :, :, :, :, :, 7, 3] = -1000\n",
    "        \n",
    "    def act(self, view, row_index, column_index, target_index_x, target_index_y, prev_action):\n",
    "        if self.eps > np.random.uniform():\n",
    "            legal_actions = []\n",
    "            action = np.random.randint(len(self.actions))\n",
    "        \n",
    "            if view[1] == 1:\n",
    "                legal_actions.append(0)\n",
    "            if view[2] == 1:\n",
    "                legal_actions.append(1)\n",
    "            if view[4] == 1:\n",
    "                legal_actions.append(2)\n",
    "            if view[7] == 1:\n",
    "                legal_actions.append(3)\n",
    "            if view[6] == 1:\n",
    "                legal_actions.append(4)\n",
    "            if view[5] == 1:\n",
    "                legal_actions.append(5)\n",
    "            if view[3] == 1:\n",
    "                legal_actions.append(6)\n",
    "            if view[0] == 1:\n",
    "                legal_actions.append(7)\n",
    "        \n",
    "            n = len(legal_actions)\n",
    "            if n != 0:\n",
    "                action = legal_actions[np.random.randint(n)]\n",
    "        \n",
    "            return action\n",
    "        \n",
    "        return np.argmax(self.q_values[tuple(view + [row_index, column_index, target_index_x, target_index_y, prev_action])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ca987",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_obstacles = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bce319",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(13, 13, max_num_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212777ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env.map_height, env.map_width, 1, 0.99999, 0.1, 0.1, 1, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "max_steps = 40\n",
    "stop_exploring_after = num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "for episode in range(num_episodes):\n",
    "    env.reset_env()\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    prev_action = 4\n",
    "    \n",
    "    if stop_exploring_after == episode:\n",
    "        agent.eps = agent.min_eps\n",
    "    \n",
    "    for step in range(1, max_steps + 1):          \n",
    "        view = env.get_view()\n",
    "        agent_pos = list(env.agent_pos)\n",
    "        target_pos = list(env.target_pos)\n",
    "        \n",
    "        \n",
    "        action = agent.act(view, agent_pos[0], agent_pos[1],\n",
    "                           target_pos[0], target_pos[1], prev_action)\n",
    "        \n",
    "        old_view = list(view)\n",
    "        old_agent_pos = list(agent_pos)\n",
    "        old_target_pos = list(target_pos)\n",
    "        old_prev_action = prev_action\n",
    "        \n",
    "        view, agent_pos, reward, done = env.step(action, prev_action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        old_q_value = agent.q_values[\n",
    "                    tuple(old_view + [old_agent_pos[0], old_agent_pos[1],\n",
    "                                      old_target_pos[0], old_target_pos[1], old_prev_action, action])]\n",
    "\n",
    "        prev_action = action\n",
    "        \n",
    "        temporal_difference = reward + agent.gamma * np.max(\n",
    "                agent.q_values[tuple(view + [agent_pos[0], agent_pos[1],\n",
    "                                            target_pos[0], target_pos[1], prev_action])]) - old_q_value\n",
    "\n",
    "        new_q_value = old_q_value + agent.lr * temporal_difference\n",
    "\n",
    "        agent.q_values[tuple(\n",
    "                    old_view + [old_agent_pos[0], old_agent_pos[1],\n",
    "                                old_target_pos[0], old_target_pos[1], old_prev_action, action])] = new_q_value\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    print('e: ' + str(episode) + ' rew: ' + str(total_reward) + ' eps: ' + str(agent.eps))\n",
    "    if agent.eps > agent.min_eps:\n",
    "        agent.eps *= agent.eps_decay\n",
    "    agent.lr *= agent.lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 10000\n",
    "agent.eps = 0.0\n",
    "\n",
    "for num_obstacles in range(max_num_obstacles + 1):\n",
    "\n",
    "    tests_passed = 0\n",
    "    \n",
    "    print('Map with ' + str(num_obstacles) + ' obstacles. eps: ' + str(agent.eps))\n",
    "    \n",
    "    for episode in range(test_episodes):\n",
    "        \n",
    "        env.map = np.array(env.map_copy)\n",
    "        env.init_env(num_obstacles)\n",
    "\n",
    "        env.init_agent_pos()\n",
    "        env.init_target_pos()\n",
    "\n",
    "        prev_action = 4\n",
    "        target_pos = env.target_pos\n",
    "\n",
    "        done = False\n",
    "\n",
    "        for step in range(1, max_steps + 1):\n",
    "            view = env.get_view()\n",
    "            agent_pos = list(env.agent_pos)\n",
    "            target_pos = list(env.target_pos)\n",
    "\n",
    "            action = agent.act(view, agent_pos[0], agent_pos[1], target_pos[0], target_pos[1], prev_action)\n",
    "\n",
    "            view, agent_pos, reward, done = env.step(action, prev_action)\n",
    "\n",
    "            prev_action = action\n",
    "\n",
    "            if done:\n",
    "                if agent_pos[0] == target_pos[0] and agent_pos[1] == target_pos[1]:\n",
    "                    tests_passed += 1\n",
    "                break\n",
    "\n",
    "    print('Success rate: ' + str(tests_passed * 100 / test_episodes) + '%\\n')\n",
    "    agent.eps += 0.011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
